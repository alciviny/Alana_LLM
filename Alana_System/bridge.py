"""
bridge.py

Este script atua como uma ponte (Sidecar) entre o Go e os modelos de IA Python.
Ele exp√µe endpoints FastAPI para embedding e gera√ß√£o de texto, mantendo os
modelos pr√©-carregados em mem√≥ria para respostas de baixa lat√™ncia.

Arquitetura: Senior Pattern (Sidecar / Hot-Start)
"""

import sys
from pathlib import Path
import logging

# Adiciona o diret√≥rio 'src' ao sys.path para encontrar o pacote 'alana_system'
src_path = Path(__file__).resolve().parent / 'src'
sys.path.insert(0, str(src_path))

from fastapi import FastAPI
from pydantic import BaseModel
from typing import List
from sentence_transformers import CrossEncoder

try:
    from alana_system.embeddings.embedder import TextEmbedder
    from alana_system.inference.llm_engine import LLMEngine
except ImportError as e:
    logging.error(f"Erro ao importar m√≥dulos do Alana System: {e}")
    logging.error("Verifique se o 'src_path' est√° correto e se o ambiente virtual est√° ativo.")
    sys.exit(1)


# =========================================================
# LOGGING
# =========================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] (Python Sidecar) %(message)s"
)
logger = logging.getLogger(__name__)


# =========================================================
# CONFIGURA√á√ïES E INICIALIZA√á√ÉO DOS MODELOS (WARM START)
# =========================================================
logger.info("Iniciando o Python Sidecar para o Alana System...")

# --- Configura√ß√µes ---
# Use as mesmas configura√ß√µes do seu script run_search.py
MODEL_PATH = "models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf"
EMBEDDER_DEVICE = "cuda" # "cuda" para GPU, "cpu" para CPU
RERANKER_DEVICE = "cuda" # "cuda" para GPU, "cpu" para CPU
LLM_GPU_LAYERS = -1      # -1 para usar o m√°ximo da GPU, 0 para CPU

# --- Carregamento dos Modelos ---
# Os modelos s√£o carregados uma √∫nica vez na inicializa√ß√£o do servidor.
try:
    logger.info("Carregando modelo de embedding...")
    embedder = TextEmbedder(device=EMBEDDER_DEVICE)
    logger.info("‚úÖ Modelo de embedding carregado.")
except Exception as e:
    logger.exception("‚ùå Falha cr√≠tica ao carregar o TextEmbedder.")
    sys.exit(1)

try:
    logger.info("Carregando modelo de Re-ranking (Cross-Encoder)...")
    # Modelo leve e r√°pido, ideal para reclassifica√ß√£o
    reranker = CrossEncoder(
        'cross-encoder/ms-marco-MiniLM-L-6-v2',
        device=RERANKER_DEVICE
    )
    logger.info("‚úÖ Modelo de Re-ranking carregado.")
except Exception as e:
    logger.exception("‚ùå Falha cr√≠tica ao carregar o CrossEncoder (Re-ranker).")
    sys.exit(1)

try:
    logger.info("Carregando modelo LLM...")
    llm = LLMEngine(
        model_path=MODEL_PATH,
        n_gpu_layers=LLM_GPU_LAYERS
    )
    logger.info("‚úÖ Modelo LLM carregado.")
except Exception as e:
    logger.exception(f"‚ùå Falha cr√≠tica ao carregar o LLMEngine. Verifique o caminho: {MODEL_PATH}")
    sys.exit(1)


# =========================================================
# API SERVER (FastAPI)
# =========================================================
app = FastAPI(
    title="Alana System - Python Sidecar",
    description="Servidor para realizar embedding, re-ranking e gera√ß√£o de texto com modelos pr√©-carregados.",
    version="1.1.0" # Vers√£o atualizada
)

# --- Defini√ß√£o dos Schemas (Contratos da API) ---
class EmbedRequest(BaseModel):
    text: str

class EmbedResponse(BaseModel):
    vector: list[float]

class RerankRequest(BaseModel):
    query: str
    documents: List[str]

class RerankResponse(BaseModel):
    scores: List[float]

class GenerateRequest(BaseModel):
    query: str
    context: str

class GenerateResponse(BaseModel):
    answer: str

# --- Endpoints da API ---
@app.post("/embed", response_model=EmbedResponse)
async def get_embedding(req: EmbedRequest):
    """Gera o embedding vetorial para um texto."""
    logger.info(f"Recebido pedido de embedding para texto: '{req.text[:50]}...'")
    vector = embedder.embed_query(req.text)
    return {"vector": vector.tolist()}

@app.post("/rerank", response_model=RerankResponse)
async def rerank_documents(req: RerankRequest):
    """
    Re-ranqueia uma lista de documentos com base na relev√¢ncia para a query,
    usando um modelo Cross-Encoder.
    """
    logger.info(f"Recebido pedido de re-ranking para query: '{req.query[:50]}...'")
    # O Cross-Encoder espera uma lista de pares: [[query, doc1], [query, doc2], ...]
    pairs = [[req.query, doc] for doc in req.documents]
    scores = reranker.predict(pairs)
    logger.info(f"Re-ranking conclu√≠do para {len(req.documents)} documentos.")
    return {"scores": scores.tolist()}

@app.post("/generate", response_model=GenerateResponse)
async def generate_answer(req: GenerateRequest):
    """Gera uma resposta com base em uma query e um contexto."""
    logger.info(f"Recebido pedido de gera√ß√£o para query: '{req.query[:50]}...'")
    answer = llm.generate_answer(query=req.query, context_text=req.context)
    return {"answer": answer}

@app.get("/health")
async def health_check():
    """Verifica se o servidor e os modelos est√£o operacionais."""
    # Uma verifica√ß√£o simples; poderia ser estendida para testar os modelos
    return {"status": "ok", "message": "Alana Sidecar est√° operacional."}


logger.info("üöÄ Servidor FastAPI pronto para receber requisi√ß√µes em http://localhost:8000")

if __name__ == "__main__":
    import uvicorn
    # Isso manter√° o servidor rodando e ouvindo na porta 8000
    uvicorn.run(app, host="127.0.0.1", port=8000)
