import logging
from typing import Optional
from llama_cpp import Llama

logger = logging.getLogger(__name__)


class LLMEngine:
    """
    Engine de LLM local usando llama.cpp

    Responsabilidades:
    - Carregar modelo local (CPU ou GPU)
    - Aplicar prompt seguro e determin√≠stico
    - Gerar respostas baseadas EXCLUSIVAMENTE no contexto
    """

    def __init__(
        self,
        model_path: str,
        context_window: int = 4096,
        n_gpu_layers: Optional[int] = None,
        seed: int = 42,
    ):
        """
        Args:
            model_path: Caminho do arquivo .gguf
            context_window: Janela total de contexto (prompt + resposta)
            n_gpu_layers:
                - None  -> auto detecta
                - 0     -> CPU
                - -1    -> tenta usar tudo da GPU
            seed: Seed fixa para respostas determin√≠sticas
        """

        if n_gpu_layers is None:
            # Fallback seguro (CPU)
            n_gpu_layers = 0

        logger.info("üîÑ Inicializando LLM local")
        logger.info(f"üì¶ Modelo: {model_path}")
        logger.info(f"üß† Context Window: {context_window}")
        logger.info(f"üéÆ GPU Layers: {n_gpu_layers}")

        self.llm = Llama(
            model_path=model_path,
            n_ctx=context_window,
            n_gpu_layers=n_gpu_layers,
            seed=seed,
            verbose=False,
        )

    def generate_answer(
        self,
        query: str,
        context_text: str,
        max_tokens: int = 512,
        temperature: float = 0.1,
    ) -> str:
        """
        Gera resposta baseada em contexto usando o formato de chat do modelo.
        """

        if not query.strip():
            raise ValueError("Query vazia")

        if not context_text.strip():
            logger.warning("‚ö†Ô∏è Contexto vazio fornecido ao LLM")
        
        system_message = """Voc√™ √© a Alana, minha assistente pessoal inteligente.

REGRAS:
- Use APENAS o contexto fornecido (minhas notas, √°udios e documentos).
- Seja direta, amig√°vel e organize as informa√ß√µes de forma √∫til.
- Sempre cite a fonte e a p√°gina/arquivo. Exemplo: (Fonte: diario.md).
- Se a informa√ß√£o n√£o estiver registrada, diga: "N√£o encontrei nada sobre isso nas minhas notas."
"""
        
        human_message = f"""CONTEXTO:
{context_text}

PERGUNTA:
{query}
"""
        
        messages = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": human_message},
        ]

        try:
            output = self.llm.create_chat_completion(
                messages=messages,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=0.9,
                repeat_penalty=1.1,
                stop=[
                    "<|eot_id|>",
                    "<|end_of_text|>",
                ],
            )

            return output["choices"][0]["message"]["content"].strip()

        except Exception as e:
            logger.exception("‚ùå Erro ao gerar resposta do LLM")
            raise RuntimeError("Falha na gera√ß√£o da resposta") from e
