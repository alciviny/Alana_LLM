import sys
import logging
from pathlib import Path
from typing import List, Dict, Any

# =========================================================
# PATH SETUP
# =========================================================
# Adiciona o diret√≥rio 'src' ao path para importar os m√≥dulos internos
sys.path.append(str(Path(__file__).parent / "src"))

from alana_system.embeddings.embedder import TextEmbedder
from alana_system.memory.vector_store import VectorStore
from alana_system.query.query_engine import QueryEngine
from alana_system.inference.llm_engine import LLMEngine

# =========================================================
# LOGGING
# =========================================================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)

# =========================================================
# TOKEN CONTROL (SIMPLIFICADO E SEGURO)
# =========================================================
def estimate_tokens(text: str) -> int:
    """
    Estimativa simples e est√°vel:
    ~1 token ‚âà 1 palavra (bom o suficiente para controle local)
    """
    if not text:
        return 0
    return len(text.split())


def truncate_context_by_budget(
    contexts: List[Dict[str, Any]], 
    max_tokens: int
) -> str:
    """
    Junta os melhores contextos respeitando o or√ßamento m√°ximo de tokens.
    Extrai o texto corretamente do dicion√°rio e adiciona metadados (p√°gina).
    """
    selected_blocks = []
    used_tokens = 0

    for item in contexts:
        # 1. Extrair o texto e p√°gina do dicion√°rio (Corre√ß√£o do Bug)
        text = item.get("text", "")
        page = item.get("page_number", "?")
        
        # 2. Formatar o bloco para a IA saber a origem
        formatted_block = f"--- [P√°gina {page}] ---\n{text}"
        
        # 3. Calcular tokens deste bloco espec√≠fico
        block_tokens = estimate_tokens(formatted_block)

        # 4. Verificar se cabe no or√ßamento
        if used_tokens + block_tokens > max_tokens:
            logger.info(f"üõë Or√ßamento atingido. Ignorando trechos restantes.")
            break

        selected_blocks.append(formatted_block)
        used_tokens += block_tokens

    logger.info(
        f"üßÆ Contexto final montado: {used_tokens}/{max_tokens} tokens "
        f"({len(selected_blocks)} trechos utilizados)"
    )

    return "\n\n".join(selected_blocks)

# =========================================================
# MAIN
# =========================================================
def main():
    print("\n" + "=" * 60)
    print("ü§ñ ALANA SYSTEM - INICIALIZA√á√ÉO")
    print("=" * 60)

    # -----------------------------------------------------
    # CONFIGURA√á√ïES
    # -----------------------------------------------------
    # Certifique-se que este arquivo existe em 'models/'
    MODEL_PATH = "models/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf"

    N_CTX = 4096  # Tamanho total da janela do modelo

    # Defini√ß√£o rigorosa do or√ßamento de tokens
    TOKEN_BUDGET = {
        "system": 300,    # Instru√ß√µes do sistema
        "question": 100,  # Tamanho m√©dio da pergunta
        "answer": 512,    # Espa√ßo reservado para a resposta da IA
    }

    # O que sobrar √© usado para o contexto dos documentos
    MAX_CONTEXT_TOKENS = (
        N_CTX
        - TOKEN_BUDGET["system"]
        - TOKEN_BUDGET["question"]
        - TOKEN_BUDGET["answer"]
    )

    logger.info(f"üìê Or√ßamento calculado para contexto: {MAX_CONTEXT_TOKENS} tokens")

    # -----------------------------------------------------
    # RAG COMPONENTS
    # -----------------------------------------------------
    print("üìö Inicializando mem√≥ria vetorial...")
    
    # Embedder: Transforma texto em n√∫meros
    embedder = TextEmbedder(device="cpu")

    # Vector Store: Banco de dados Qdrant
    vector_store = VectorStore(
        collection_name="alana_knowledge_base",
        path="./qdrant_data"
    )

    # Query Engine: Realiza a busca sem√¢ntica
    query_engine = QueryEngine(
        embedder=embedder,
        vector_store=vector_store,
        top_k=5,              # Busca at√© 5 trechos iniciais
        score_threshold=0.35  # Filtra resultados irrelevantes
    )

    # -----------------------------------------------------
    # LLM (C√©rebro)
    # -----------------------------------------------------
    print(f"üß† Carregando LLM local: {MODEL_PATH}")
    try:
        llm = LLMEngine(
            model_path=MODEL_PATH,
            context_window=N_CTX,
            n_gpu_layers=-1  # 0 = CPU, -1 = GPU (se dispon√≠vel e configurado)
        )
    except Exception as e:
        logger.error("‚ùå Falha cr√≠tica ao carregar modelo LLM")
        logger.error(f"Detalhe do erro: {e}")
        logger.error("DICA: Verifique se o arquivo .gguf est√° na pasta 'models/'")
        return

    print("\n" + "=" * 60)
    print("‚úÖ ALANA ONLINE ‚Äî Pergunte sobre seus documentos")
    print("=" * 60)

    # -----------------------------------------------------
    # LOOP DE CONVERSA
    # -----------------------------------------------------
    while True:
        try:
            question = input("\nVoc√™: ").strip()
        except KeyboardInterrupt:
            print("\nüëã Encerrando Alana.")
            break

        if question.lower() in {"sair", "exit", "quit"}:
            print("üëã Encerrando Alana.")
            break

        if not question:
            print("‚ö†Ô∏è Pergunta vazia.")
            continue

        # 1. Recupera√ß√£o (Retrieval)
        logger.info("üîç Buscando contexto relevante...")
        search_result = query_engine.query(question)
        
        # O QueryEngine retorna uma lista de dicts em 'contexts'
        raw_contexts = search_result.get("contexts", [])

        if not raw_contexts:
            print("\n‚ùå Alana: N√£o encontrei informa√ß√µes relevantes nos documentos para responder isso.")
            continue

        # 2. Controle de Tokens e Formata√ß√£o
        #    Aqui usamos a fun√ß√£o corrigida que l√™ os dicion√°rios
        context_text = truncate_context_by_budget(
            contexts=raw_contexts,
            max_tokens=MAX_CONTEXT_TOKENS
        )

        # 3. Gera√ß√£o (Generation)
        logger.info("ü§î Gerando resposta...")
        try:
            answer = llm.generate_answer(
                query=question,
                context_text=context_text,
                max_tokens=TOKEN_BUDGET["answer"],
                temperature=0.1
            )

            print(f"\nü§ñ Alana:\n{answer}")
            print(
                f"\n[Fonte: {len(raw_contexts)} trechos encontrados | "
                f"Contexto usado: {estimate_tokens(context_text)} tokens]"
            )

        except Exception as e:
            logger.error("‚ùå Erro durante infer√™ncia")
            print(f"\nErro t√©cnico: {e}")

# =========================================================
# ENTRYPOINT
# =========================================================
if __name__ == "__main__":
    main()